# routers/chat.py (Corrected Imports)
import logging
from fastapi import APIRouter, HTTPException, Depends
# Keep Pydantic import if needed for local models, but ChatRequest/Response are moved
# from pydantic import BaseModel, Field

# --- Corrected Imports ---
# Use direct imports assuming backend/ is in PYTHONPATH when run
import main_config 
import main_models 
# --- End Corrected Imports ---

# Langchain and other imports remain the same
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import Runnable
from langchain_core.exceptions import OutputParserException
import httpx

# Use a logger specific to this module
logger = logging.getLogger(__name__)

# --- Langchain Chain Setup ---
# Use a dependency function to create/provide the chain.
# This makes testing easier and manages the chain's lifecycle.
def get_chat_chain() -> Runnable:
    """
    Creates and returns the Langchain chat chain using configured Ollama settings.
    Raises a RuntimeError if configuration is missing or chain initialization fails,
    which will be caught by FastAPI during dependency resolution.
    """
    # --- Configuration Check ---
    # Ensure Ollama configuration was loaded correctly from main_config.py
    # Use direct access to the imported module
    if main_config.OLLAMA_BASE_CHAT_MODEL == "missing_model": # Using the actual variable name from config
         logger.error("Ollama configuration (OLLAMA_BASE_CHAT_MODEL/OLLAMA_BASE_URL) is missing or invalid. Cannot create chat chain.")
         # Raise an error that FastAPI's dependency injection can catch, resulting in a 500 error for requests.
         raise RuntimeError("Chatbot service is not configured correctly (missing Ollama model/URL).")

    try:
        # --- Initialize ChatOllama ---
        # Use the imported configuration variables.
        llm = ChatOllama(
            model=main_config.OLLAMA_BASE_CHAT_MODEL, # Using the actual variable name from config
            base_url=main_config.OLLAMA_BASE_URL
            # Add other Ollama parameters if needed (e.g., temperature, top_k, top_p)
            # temperature=0.7,
        )

        # --- Define Prompt Template ---
        # System message provides context to the LLM.
        # "{user_message}" is the placeholder for the user's input.
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are SlySearch AI, a helpful and concise assistant integrated into the SlySearch application environment."),
            ("user", "{user_message}")
        ])

        # --- Define Output Parser ---
        # StrOutputParser simply returns the string content from the LLM response.
        output_parser = StrOutputParser()

        # --- Construct Chain using LCEL ---
        # Pipe the components together: Prompt -> LLM -> Output Parser
        chain = prompt | llm | output_parser
        logger.info("Langchain chat chain created successfully.")
        return chain

    except Exception as e:
        # Catch any other errors during LLM/chain initialization (e.g., Ollama library issues)
        logger.exception(f"Failed to initialize Langchain ChatOllama chain: {e}")
        # Raise RuntimeError to signal failure during dependency setup.
        raise RuntimeError(f"Could not create Langchain chat chain. Is Ollama configured correctly and running? Error: {e}")

# --- API Router Definition ---
# Create an APIRouter instance specific to this chat functionality.
router = APIRouter()

# --- Chat Endpoint Implementation ---
@router.post(
    "/chat", # Actual path will be "/api/v1/chat" due to prefix in main.py
    response_model=main_models.ChatResponse, # Use the imported Pydantic response model via module
    summary="Send a message to the AI Chatbot",
    description="Receives a user message and returns a stateless response generated by the configured Ollama LLM via Langchain.",
    # Tags are applied in main.py when including the router
)
async def handle_chat(
    request: main_models.ChatRequest, # Use the imported Pydantic request model via module
    # Use FastAPI's Depends to inject the chat chain created by get_chat_chain.
    # If get_chat_chain raises an error, FastAPI handles it before this function runs.
    chat_chain: Runnable = Depends(get_chat_chain)
):
    """
    Handles incoming chat messages, invokes the Langchain chain, and returns the response.
    """
    logger.info(f"Received chat message via v1 API: '{request.message[:100]}...'") # Log sanitized message

    try:
        # --- Invoke Langchain Chain ---
        # Asynchronously invoke the chain with the user's message.
        # The dictionary key must match the placeholder in the ChatPromptTemplate ("user_message").
        response_content = await chat_chain.ainvoke({"user_message": request.message})

        # Log the generated response (sanitized)
        logger.info(f"Generated chat response: '{str(response_content)[:100]}...'")

        # --- Return Success Response ---
        # Wrap the string response in the Pydantic response model.
        return main_models.ChatResponse(response=str(response_content)) # Ensure response is string

    # --- Error Handling during Chain Invocation ---
    except (OutputParserException) as e:
         # Handle errors specifically related to parsing the LLM's output (if using more complex parsers later)
         logger.error(f"Langchain output parsing error: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail=f"Chatbot failed to parse the AI response.")
    except (httpx.ConnectError, httpx.TimeoutException, httpx.RequestError) as e:
        # Handle network errors when communicating with the Ollama service.
        logger.error(f"Could not connect to Ollama service at {main_config.OLLAMA_BASE_URL}: {e}", exc_info=False) # Keep log cleaner for common network issues
        raise HTTPException(status_code=503, detail=f"Could not connect to the AI service (Ollama). Please check if it's running and accessible at {main_config.OLLAMA_BASE_URL}.")
    except Exception as e:
        # Catch any other unexpected errors during the chain invocation.
        logger.error(f"Unexpected error during chat generation: {e}", exc_info=True)

        # Try to provide a more specific error message if possible (e.g., model not found errors from Ollama)
        error_str = str(e).lower()
        if "model not found" in error_str or main_config.OLLAMA_BASE_CHAT_MODEL == "missing_model":
             detail = f"Chatbot model '{main_config.OLLAMA_BASE_CHAT_MODEL}' not found or configuration missing. Please check backend logs and ensure the model is available in Ollama."
             status_code = 500 # Internal configuration error
        elif "connection refused" in error_str: # Duplicate check, but catches different phrasing
             detail = f"Connection refused by AI service (Ollama) at {main_config.OLLAMA_BASE_URL}. Is it running?"
             status_code = 503 # Service Unavailable
        else:
            detail = f"An internal error occurred in the chatbot processing."
            status_code = 500 # Generic Internal Server Error

        raise HTTPException(status_code=status_code, detail=detail)
